{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonbuchanan145/Data-Modeling/blob/main/hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l76esdmAzN5U",
        "outputId": "cbaf63f3-f93c-404f-f1b6-79aa33eaeb48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Collecting py-solc-x\n",
            "  Downloading py_solc_x-2.0.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from py-solc-x) (2.32.3)\n",
            "Collecting packaging<24,>=23.1 (from py-solc-x)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.19.0->py-solc-x) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.19.0->py-solc-x) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.19.0->py-solc-x) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.19.0->py-solc-x) (2025.1.31)\n",
            "Downloading py_solc_x-2.0.3-py3-none-any.whl (18 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, py-solc-x\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-23.2 py-solc-x-2.0.3\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting slither-analyzer\n",
            "  Downloading slither_analyzer-0.11.0-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from slither-analyzer) (23.2)\n",
            "Requirement already satisfied: prettytable>=3.10.2 in /usr/local/lib/python3.11/dist-packages (from slither-analyzer) (3.16.0)\n",
            "Collecting pycryptodome>=3.4.6 (from slither-analyzer)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting crytic-compile<0.4.0,>=0.3.8 (from slither-analyzer)\n",
            "  Downloading crytic_compile-0.3.8-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting web3<7,>=6.20.2 (from slither-analyzer)\n",
            "  Downloading web3-6.20.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting eth-abi>=4.0.0 (from slither-analyzer)\n",
            "  Downloading eth_abi-5.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting eth-typing>=3.0.0 (from slither-analyzer)\n",
            "  Downloading eth_typing-5.2.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting eth-utils>=2.1.0 (from slither-analyzer)\n",
            "  Downloading eth_utils-5.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting cbor2 (from crytic-compile<0.4.0,>=0.3.8->slither-analyzer)\n",
            "  Downloading cbor2-5.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting solc-select>=v1.0.4 (from crytic-compile<0.4.0,>=0.3.8->slither-analyzer)\n",
            "  Downloading solc_select-1.0.4-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting parsimonious<0.11.0,>=0.10.0 (from eth-abi>=4.0.0->slither-analyzer)\n",
            "  Downloading parsimonious-0.10.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from eth-typing>=3.0.0->slither-analyzer) (4.13.0)\n",
            "Collecting eth-hash>=0.3.1 (from eth-utils>=2.1.0->slither-analyzer)\n",
            "  Downloading eth_hash-0.7.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting cytoolz>=0.10.1 (from eth-utils>=2.1.0->slither-analyzer)\n",
            "  Downloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable>=3.10.2->slither-analyzer) (0.2.13)\n",
            "Requirement already satisfied: aiohttp>=3.7.4.post0 in /usr/local/lib/python3.11/dist-packages (from web3<7,>=6.20.2->slither-analyzer) (3.11.15)\n",
            "Collecting ckzg<2 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading ckzg-1.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (683 bytes)\n",
            "Collecting eth-account<0.13,>=0.8.0 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_account-0.12.4-py3-none-any.whl.metadata (5.1 kB)\n",
            "INFO: pip is looking at multiple versions of web3 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting web3<7,>=6.20.2 (from slither-analyzer)\n",
            "  Downloading web3-6.20.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Downloading web3-6.20.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting eth-utils>=2.1.0 (from slither-analyzer)\n",
            "  Downloading eth_utils-5.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "  Downloading eth_utils-5.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting hexbytes>=1.0.0 (from eth-utils>=2.1.0->slither-analyzer)\n",
            "  Downloading hexbytes-1.3.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "INFO: pip is still looking at multiple versions of web3 to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting eth-utils>=2.1.0 (from slither-analyzer)\n",
            "  Downloading eth_utils-4.1.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting eth-typing>=3.0.0 (from slither-analyzer)\n",
            "  Downloading eth_typing-4.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting hexbytes<0.4.0,>=0.1.0 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading hexbytes-0.3.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: jsonschema>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from web3<7,>=6.20.2->slither-analyzer) (4.23.0)\n",
            "Collecting lru-dict<1.3.0,>=1.1.6 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading lru_dict-1.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: protobuf>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from web3<7,>=6.20.2->slither-analyzer) (5.29.4)\n",
            "Requirement already satisfied: requests>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from web3<7,>=6.20.2->slither-analyzer) (2.32.3)\n",
            "Collecting websockets<14.0.0,>=10.0.0 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pyunormalize>=15.0.0 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading pyunormalize-16.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.4.post0->web3<7,>=6.20.2->slither-analyzer) (1.18.3)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->eth-utils>=2.1.0->slither-analyzer) (0.12.1)\n",
            "Collecting bitarray>=2.4.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading bitarray-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting eth-keyfile>=0.6.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_keyfile-0.9.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting eth-keys>=0.4.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_keys-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting eth-rlp>=2.1.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_rlp-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "INFO: pip is looking at multiple versions of eth-account to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting eth-account<0.13,>=0.8.0 (from web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_account-0.12.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading eth_account-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading eth_account-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading eth_account-0.12.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading eth_account-0.11.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting rlp>=1.0.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading rlp-4.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.0->web3<7,>=6.20.2->slither-analyzer) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.0->web3<7,>=6.20.2->slither-analyzer) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.0.0->web3<7,>=6.20.2->slither-analyzer) (0.24.0)\n",
            "Requirement already satisfied: regex>=2022.3.15 in /usr/local/lib/python3.11/dist-packages (from parsimonious<0.11.0,>=0.10.0->eth-abi>=4.0.0->slither-analyzer) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->web3<7,>=6.20.2->slither-analyzer) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->web3<7,>=6.20.2->slither-analyzer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->web3<7,>=6.20.2->slither-analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->web3<7,>=6.20.2->slither-analyzer) (2025.1.31)\n",
            "Collecting py_ecc>=5.2.0 (from eth-keyfile>=0.6.0->eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading py_ecc-7.0.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "INFO: pip is looking at multiple versions of eth-rlp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting eth-rlp>=0.3.0 (from eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading eth_rlp-2.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Downloading eth_rlp-2.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "  Downloading eth_rlp-1.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting cached-property>=1.5.1 (from py_ecc>=5.2.0->eth-keyfile>=0.6.0->eth-account<0.13,>=0.8.0->web3<7,>=6.20.2->slither-analyzer)\n",
            "  Downloading cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading slither_analyzer-0.11.0-py3-none-any.whl (814 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.7/814.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crytic_compile-0.3.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_abi-5.2.0-py3-none-any.whl (28 kB)\n",
            "Downloading eth_utils-4.1.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading web3-6.20.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_typing-4.4.0-py3-none-any.whl (19 kB)\n",
            "Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ckzg-1.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_account-0.11.3-py3-none-any.whl (355 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.4/355.4 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_hash-0.7.1-py3-none-any.whl (8.0 kB)\n",
            "Downloading hexbytes-0.3.1-py3-none-any.whl (5.9 kB)\n",
            "Downloading lru_dict-1.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyunormalize-16.0.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading solc_select-1.0.4-py3-none-any.whl (20 kB)\n",
            "Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cbor2-5.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eth_keyfile-0.9.1-py3-none-any.whl (9.9 kB)\n",
            "Downloading eth_keys-0.6.1-py3-none-any.whl (21 kB)\n",
            "Downloading eth_rlp-1.0.1-py3-none-any.whl (4.9 kB)\n",
            "Downloading rlp-4.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading py_ecc-7.0.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cached_property-2.0.1-py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: lru-dict, ckzg, bitarray, websockets, pyunormalize, pycryptodome, parsimonious, hexbytes, eth-typing, eth-hash, cytoolz, cbor2, cached-property, solc-select, eth-utils, rlp, py_ecc, eth-keys, eth-abi, crytic-compile, eth-rlp, eth-keyfile, eth-account, web3, slither-analyzer\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "Successfully installed bitarray-3.3.1 cached-property-2.0.1 cbor2-5.6.5 ckzg-1.0.2 crytic-compile-0.3.8 cytoolz-1.0.1 eth-abi-5.2.0 eth-account-0.11.3 eth-hash-0.7.1 eth-keyfile-0.9.1 eth-keys-0.6.1 eth-rlp-1.0.1 eth-typing-4.4.0 eth-utils-4.1.1 hexbytes-0.3.1 lru-dict-1.2.0 parsimonious-0.10.0 py_ecc-7.0.1 pycryptodome-3.22.0 pyunormalize-16.0.0 rlp-4.1.0 slither-analyzer-0.11.0 solc-select-1.0.4 web3-6.20.4 websockets-13.1\n",
            "Installing solc '0.8.28'...\n",
            "Version '0.8.28' installed.\n",
            "Collecting trl\n",
            "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.5.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.50.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.30.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.11.15)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.16.1\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Switched global version to 0.8.28\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install datasets\n",
        "!pip install huggingface_hub\n",
        "!pip install py-solc-x\n",
        "!pip install -U bitsandbytes\n",
        "!pip install slither-analyzer\n",
        "!solc-select install 0.8.28\n",
        "!pip install trl\n",
        "!pip install tensorboard\n",
        "!solc-select use 0.8.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQCMB6yjG_zI",
        "outputId": "f035d1de-7219-4902-afba-b00aa54d28af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/solidity_project\n",
            "\u001b[1G\u001b[0KWrote to /content/solidity_project/package.json:\n",
            "\n",
            "{\n",
            "  \"name\": \"solidity_project\",\n",
            "  \"version\": \"1.0.0\",\n",
            "  \"main\": \"index.js\",\n",
            "  \"scripts\": {\n",
            "    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n",
            "  },\n",
            "  \"keywords\": [],\n",
            "  \"author\": \"\",\n",
            "  \"license\": \"ISC\",\n",
            "  \"description\": \"\"\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@8.1.0: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m ethereumjs-abi@0.6.8: This library has been deprecated and usage is discouraged.\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m lodash.isequal@4.5.0: This package is deprecated. Use require('node:util').isDeepStrictEqual instead.\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@7.2.3: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@7.2.3: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@5.0.15: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@7.1.7: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m inflight@1.0.6: This module is not supported, and leaks memory. Do not use it. Check out lru-cache if you want a good and tested way to coalesce async requests by a key value, which is much more comprehensive and powerful.\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "added 582 packages, and audited 583 packages in 26s\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K99 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "13 \u001b[1mlow\u001b[22m severity vulnerabilities\n",
            "\n",
            "To address issues that do not require attention, run:\n",
            "  npm audit fix\n",
            "\n",
            "Some issues need review, and may require choosing\n",
            "a different dependency.\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mskipping integrity check for git dependency ssh://git@github.com/matter-labs/era-contracts.git\u001b[39m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m rimraf@2.7.1: Rimraf versions prior to v4 are no longer supported\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[33mwarn\u001b[39m \u001b[94mdeprecated\u001b[39m glob@7.2.3: Glob versions prior to v9 are no longer supported\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "added 151 packages, and audited 734 packages in 2m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K108 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[31m\u001b[1m19\u001b[22m\u001b[39m vulnerabilities (14 \u001b[1mlow\u001b[22m, 5 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m)\n",
            "\n",
            "To address issues that do not require attention, run:\n",
            "  npm audit fix\n",
            "\n",
            "Some issues need review, and may require choosing\n",
            "a different dependency.\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create a working directory for our Solidity project\n",
        "!mkdir -p solidity_project\n",
        "%cd solidity_project\n",
        "\n",
        "# Initialize npm project and install dependencies\n",
        "!npm init -y\n",
        "!npm install --save-dev hardhat @nomicfoundation/hardhat-toolbox @openzeppelin/contracts\n",
        "!npm install @chainlink/contracts # Add other common dependencies as needed\n",
        "\n",
        "\n",
        "# Create contracts directory\n",
        "!mkdir -p contracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QVVGi3HtF2E-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "log = logging.getLogger(__name__)\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a file handler and set the filename\n",
        "file_handler = logging.FileHandler('/content/logger.log')\n",
        "\n",
        "# Create a formatter and add it to the handler\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the logger\n",
        "log.addHandler(file_handler)\n",
        "log.propagate = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YOZd8PRsVkc"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import glob\n",
        "import subprocess\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "# PyTorch and related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.amp import autocast\n",
        "from torch.amp import GradScaler\n",
        "from datasets import load_dataset, Dataset as HFDataset # Import HFDataset from datasets\n",
        "# Transformers and TRL\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TFAutoModelForMaskedLM,\n",
        "    GenerationConfig,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import (\n",
        "    PPOTrainer,\n",
        "    PPOConfig,\n",
        "    AutoModelForCausalLMWithValueHead,\n",
        "    create_reference_model\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Solidity Compiler\n",
        "from solcx import compile_standard, install_solc\n",
        "\n",
        "# BitsAndBytes\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "# Set the path to save the best model\n",
        "output_dir = \"/content/drive/MyDrive/llama_generated_contracts/explanations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "953a1d73565d4ff2a6175110879d4a86",
            "a6696682e07546a39db0a0f676852a42",
            "8f70233c7a7f4d33b0cbcb056887a3f6",
            "9ffbc210ac7c40909b63222ed9308c9c",
            "1f2eaf115ecd40259e7eec6a3b6cb2d8",
            "36085067997d4e089bea01ba0e879778",
            "ee3bfdedd51441159cd349939382d11e",
            "de08b46095ac41fcb08f43855f7eb806",
            "88741e69e6cb4a1bafae1f94f062da07",
            "f0ae6819871d4e988eacea63bc70826b",
            "ada1ca99aece4224bd91c985564e41b1",
            "4515efe5f11946d398a4038245ac763f",
            "d11f82dcad1a48ab908202bd46e14f0f"
          ]
        },
        "id": "fO09wVXpsg5h",
        "outputId": "98273d96-ee9e-4146-f508-076280446bf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "953a1d73565d4ff2a6175110879d4a86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6696682e07546a39db0a0f676852a42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f70233c7a7f4d33b0cbcb056887a3f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ffbc210ac7c40909b63222ed9308c9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f2eaf115ecd40259e7eec6a3b6cb2d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36085067997d4e089bea01ba0e879778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee3bfdedd51441159cd349939382d11e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de08b46095ac41fcb08f43855f7eb806",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88741e69e6cb4a1bafae1f94f062da07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0ae6819871d4e988eacea63bc70826b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ada1ca99aece4224bd91c985564e41b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4515efe5f11946d398a4038245ac763f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d11f82dcad1a48ab908202bd46e14f0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'Qwen/Qwen2.5-Coder-7B-instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "model_name = \"Qwen/Qwen2.5-Coder-7B-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "2e53580f70294d7899aa70696164a32b"
          ]
        },
        "id": "wQtDMrmEK5qJ",
        "outputId": "947b0511-d80f-4ccf-e4a2-c8f328c50300"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e53580f70294d7899aa70696164a32b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'Qwen/Qwen2.5-Coder-7B-instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
          ]
        }
      ],
      "source": [
        "# Create a new model instance on CPU\n",
        "with torch.device(\"cpu\"):\n",
        "    reference_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"cpu\"\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CnYRhVvuTk96"
      },
      "outputs": [],
      "source": [
        "def make_prompts(file_path):\n",
        "  dataset = []\n",
        "  with open(file_path, 'r') as file:\n",
        "      for line in file:\n",
        "          line = line.strip()\n",
        "          if line.startswith('\"') and line.endswith('\"'):\n",
        "              # Remove the surrounding quotes\n",
        "              clean_line = line[1:-1]\n",
        "              clean_line = \"Using Solidity and having your code start with \\\"// Begin Smart Contract\\\" and ending with \\\"//End Smart Contract\\\" as comments \"+clean_line\n",
        "              dataset.append(clean_line)\n",
        "  return dataset\n",
        "prompts = make_prompts(\"/content/drive/MyDrive/prompts.txt\")\n",
        "validation_prompts = make_prompts(\"/content/drive/MyDrive/validation_prompts.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CgoLILWYOhmq"
      },
      "outputs": [],
      "source": [
        "def setup_hardhat_environment():\n",
        "    \"\"\"\n",
        "    Set up the initial Hardhat environment with common dependencies.\n",
        "    Run this once at the beginning of your notebook.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import os\n",
        "\n",
        "    # Create directories relative to the project root\n",
        "    os.makedirs('./contracts', exist_ok=True)\n",
        "    os.makedirs('./artifacts', exist_ok=True)\n",
        "\n",
        "    log.info(\"Installing common Solidity dependencies...\")\n",
        "    dependencies = [\n",
        "        \"hardhat\",\n",
        "        \"@nomicfoundation/hardhat-toolbox\",\n",
        "        \"@openzeppelin/contracts\",\n",
        "        \"@chainlink/contracts\"\n",
        "    ]\n",
        "\n",
        "    install_cmd = [\"npm\", \"install\", \"--save-dev\"] + dependencies\n",
        "    process = subprocess.run(install_cmd, capture_output=True, text=True)\n",
        "\n",
        "    if process.returncode != 0:\n",
        "        log.info(f\"Error installing dependencies: {process.stderr}\")\n",
        "        return False\n",
        "\n",
        "    # Create hardhat.config.js with relative paths for sources and artifacts\n",
        "    hardhat_config = '''\n",
        "module.exports = {\n",
        "  solidity: {\n",
        "    version: \"0.8.20\",\n",
        "    settings: {\n",
        "      optimizer: {\n",
        "        enabled: true,\n",
        "        runs: 200\n",
        "      },\n",
        "      evmVersion: \"paris\"\n",
        "    }\n",
        "  },\n",
        "  paths: {\n",
        "    sources: \"./contracts\",\n",
        "    artifacts: \"./artifacts\"\n",
        "  },\n",
        "  networks: {\n",
        "    hardhat: {\n",
        "      chainId: 1337\n",
        "    }\n",
        "  },\n",
        "  mocha: {\n",
        "    timeout: 40000\n",
        "  }\n",
        "};\n",
        "'''\n",
        "\n",
        "    with open('hardhat.config.js', 'w') as f:\n",
        "        f.write(hardhat_config)\n",
        "\n",
        "    log.info(\"Hardhat environment setup complete\")\n",
        "    return True\n",
        "\n",
        "setup_hardhat_environment()\n",
        "def evaluate_contract_hard(contract_source, full_response):\n",
        "    \"\"\"\n",
        "    Evaluate a Solidity contract by resolving additional dependencies,\n",
        "    compiling it with Hardhat, and validating the generated artifact.\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"reward\": int (10 for success, negative for errors),\n",
        "            \"error_token_indices\": list of token indices corresponding to problematic error snippets\n",
        "        }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Write the contract source to a file in the contracts directory\n",
        "        contract_path = Path('./contracts/MainContract.sol')\n",
        "        with open(contract_path, 'w') as f:\n",
        "            f.write(contract_source)\n",
        "\n",
        "        # Detect contract name\n",
        "        contract_name_match = re.search(r'contract\\s+(\\w+)(?:\\s+is\\s+[^{]+)?\\s*{', contract_source)\n",
        "        if contract_name_match:\n",
        "            contract_name = contract_name_match.group(1)\n",
        "            log.info(f\"Detected contract name: {contract_name}\")\n",
        "        else:\n",
        "            contract_name = \"MainContract\"\n",
        "            log.info(f\"Could not detect contract name, using default: {contract_name}\")\n",
        "\n",
        "        # Detect import statements that may require additional npm packages\n",
        "        import_pattern = re.compile(r'import\\s+(?:{[^}]*}\\s+from\\s+)?[\\'\"](.+?)[\\'\"];')\n",
        "        imports = import_pattern.findall(contract_source)\n",
        "\n",
        "        # Map import prefixes to their corresponding npm package names\n",
        "        dependency_map = {\n",
        "            '@openzeppelin/': '@openzeppelin/contracts',\n",
        "            '@chainlink/': '@chainlink/contracts',\n",
        "            '@uniswap/': '@uniswap/v3-core',\n",
        "            'hardhat/': '@nomicfoundation/hardhat-toolbox',\n",
        "            '@aave/': '@aave/protocol-v2',\n",
        "            '@compound-finance/': '@compound-finance/contracts',\n",
        "            '@balancer-labs/': '@balancer-labs/v2-solidity-utils',\n",
        "            '@1inch/': '@1inch/solidity-utils',\n",
        "            '@sushiswap/': '@sushiswap/core'\n",
        "        }\n",
        "\n",
        "        dependencies_to_install = set()\n",
        "        for imp in imports:\n",
        "            for prefix, package in dependency_map.items():\n",
        "                if prefix in imp:\n",
        "                    dependencies_to_install.add(package)\n",
        "\n",
        "        if dependencies_to_install:\n",
        "            log.info(f\"Installing additional dependencies: {', '.join(dependencies_to_install)}\")\n",
        "            install_cmd = ['npm', 'install', '--save-dev'] + list(dependencies_to_install)\n",
        "            install_process = subprocess.run(install_cmd, capture_output=True, text=True)\n",
        "            if install_process.returncode != 0:\n",
        "                log.info(f\"Warning: Failed to install some dependencies: {install_process.stderr}\")\n",
        "\n",
        "        # Clean the environment and compile the contract\n",
        "        subprocess.run(['npx', 'hardhat', 'clean'], capture_output=True)\n",
        "        compile_process = subprocess.run(\n",
        "            ['npx', 'hardhat', 'compile', '--force'],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        # If compilation fails, extract error token indices\n",
        "        if compile_process.returncode != 0:\n",
        "            log.info(\"Compilation failed\")\n",
        "            log.info(f\"STDOUT: {compile_process.stdout}\")\n",
        "            log.info(f\"STDERR: {compile_process.stderr}\")\n",
        "\n",
        "            # Extract error snippets using regex matching lines starting with a number then \"|\"\n",
        "            stderr = compile_process.stderr\n",
        "            pattern = r'^\\s*\\d+\\s*\\|\\s*(.+)$'\n",
        "            error_snippets = re.findall(pattern, stderr, re.MULTILINE)\n",
        "            log.info(f\"Extracted error snippets: {error_snippets}\")\n",
        "            error_token_indices = []\n",
        "            # Use offset mapping for precise mapping.\n",
        "            encoded = tokenizer(full_response, return_offsets_mapping=True)\n",
        "            offsets = encoded.get('offset_mapping', None)\n",
        "            if offsets and isinstance(offsets[0], int):\n",
        "                # This is a single list of integers; wrap it.\n",
        "                offsets = [tuple(offsets)]\n",
        "            elif offsets and isinstance(offsets[0], (list, tuple)) and isinstance(offsets[0][0], int):\n",
        "                # Offsets are already in the correct format; do nothing.\n",
        "                pass\n",
        "            else:\n",
        "              log.info(\"Offset mapping not available or in unexpected format: %s. Falling back to cumulative-length mapping.\", repr(offsets))\n",
        "              # Fallback: create offsets by cumulative token lengths.\n",
        "              tokens = tokenizer(full_response, return_tensors=\"pt\").input_ids[0]\n",
        "              decoded_tokens = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
        "              offsets = []\n",
        "              current_pos = 0\n",
        "              for token in decoded_tokens:\n",
        "                  token_len = len(token)\n",
        "                  offsets.append((current_pos, current_pos + token_len))\n",
        "                  current_pos += token_len\n",
        "            # For each error snippet, find its character span and then map to token indices.\n",
        "            for snippet in error_snippets:\n",
        "                snippet_start = full_response.find(snippet)\n",
        "                if snippet_start != -1:\n",
        "                    snippet_end = snippet_start + len(snippet)\n",
        "                    for i, (token_start, token_end) in enumerate(offsets):\n",
        "                        if token_end > snippet_start and token_start < snippet_end:\n",
        "                            error_token_indices.append(i)\n",
        "\n",
        "            error_token_indices = sorted(set(error_token_indices))\n",
        "            return {\"reward\": -1, \"error_token_indices\": error_token_indices}\n",
        "\n",
        "        # Verify that the artifact was created\n",
        "        artifact_dir = Path('artifacts/contracts/MainContract.sol')\n",
        "        artifact_files = list(artifact_dir.glob('*.json'))\n",
        "        if not artifact_files:\n",
        "            log.info(\"Compilation appeared successful but no artifacts were generated\")\n",
        "            return {\"reward\": 5, \"error_token_indices\": []}\n",
        "\n",
        "        log.info(\"Contract compiled successfully\")\n",
        "        return {\"reward\": 0.01, \"error_token_indices\": []}\n",
        "\n",
        "    except Exception as e:\n",
        "        log.info(f\"Error during contract evaluation: {str(e)}\")\n",
        "        return {\"reward\": -20, \"error_token_indices\": []}\n",
        "\n",
        "    finally:\n",
        "        import shutil\n",
        "        contract_path = Path('contracts/MainContract.sol')\n",
        "        if contract_path.exists():\n",
        "            os.remove(contract_path)\n",
        "        artifacts_path = Path('artifacts/contracts/MainContract.sol')\n",
        "        if artifacts_path.exists():\n",
        "            shutil.rmtree(artifacts_path)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_contract(contract_source):\n",
        "    return evaluate_contract_hard(contract_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC-dPwipK9hS"
      },
      "outputs": [],
      "source": [
        "model.generation_config = GenerationConfig.from_model_config(model.config)\n",
        "\n",
        "train_dataset = HFDataset.from_dict({\"query\": prompts})\n",
        "\n",
        "# Define a prompt processor class (required by PPOTrainer)\n",
        "class PromptProcessor:\n",
        "    def __init__(self, processing_class):\n",
        "        self.processing_class = processing_class\n",
        "\n",
        "    def process_prompt(self, prompt):\n",
        "        \"\"\"Format prompt according to official Qwen pattern\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Solidity developer specialized in writing secure, efficient smart contracts. These contracts are intended to test your capabilities so ensure that you always provide complete, working code with no placeholders.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        # Apply chat template exactly as in quickstart\n",
        "        text = self.processing_class.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Create inputs as shown in quickstart\n",
        "        model_inputs = self.processing_class([text], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "        return model_inputs\n",
        "\n",
        "    def generate_response(self, prompt_tensors, **generation_kwargs):\n",
        "        # Define generation parameters\n",
        "        gen_kwargs = {\n",
        "            \"max_new_tokens\": 2000,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.95,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1\n",
        "        }\n",
        "        gen_kwargs.update(generation_kwargs)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.amp.autocast(\"cuda\"):  # Mixed precision for efficiency\n",
        "            response = model.generate(**prompt_tensors, **gen_kwargs)\n",
        "\n",
        "        return response\n",
        "\n",
        "# Initialize processor with your tokenizer\n",
        "prompt_processor = PromptProcessor(tokenizer)\n",
        "\n",
        "formatted_prompts = [{\"prompt\": prompt} for prompt in prompts]\n",
        "train_dataset = HFDataset.from_list(formatted_prompts)\n",
        "\n",
        "ppo_config = PPOConfig(\n",
        "    learning_rate=1e-5,\n",
        "    batch_size=1,\n",
        "    mini_batch_size=1,\n",
        "    num_ppo_epochs=4,\n",
        "    gamma=1.0,\n",
        "    lam=0.95,\n",
        "    cliprange=0.2,\n",
        "    cliprange_value=0.2,\n",
        "    vf_coef=0.1,\n",
        "    seed=42,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "class CustomPPOTrainer(PPOTrainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize without calling parent init as before\n",
        "        from transformers import TrainingArguments\n",
        "        self.args = kwargs.pop('args', None) or TrainingArguments(\"./output\")\n",
        "        self.policy_model = kwargs.pop('model')\n",
        "        self.ref_model = kwargs.pop('ref_model')\n",
        "        self.train_dataset = kwargs.pop('train_dataset')\n",
        "        self.processing_class = kwargs.pop('processing_class')\n",
        "        self.policy_model.gradient_checkpointing_enable()\n",
        "        self.setup_data_loader()\n",
        "        # Initialize the GradScaler for mixed precision training\n",
        "        self.scaler = GradScaler()\n",
        "        self.setup_optimizers()  # Modified to use a memory-efficient optimizer\n",
        "\n",
        "    def setup_data_loader(self):\n",
        "        from torch.utils.data import DataLoader\n",
        "        self.dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    def setup_optimizers(self):\n",
        "        # Instead of torch.optim.Adam, use the 8-bit Adam optimizer from bitsandbytes\n",
        "        self.optimizer = Adam8bit(\n",
        "            self.policy_model.parameters(),\n",
        "            lr=self.args.learning_rate\n",
        "        )\n",
        "\n",
        "    def step(self, prompts, responses, rewards):\n",
        "          # Ensure correct data type for responses and prompts\n",
        "          if hasattr(responses, 'dtype') and responses.dtype == torch.float16:\n",
        "              responses = responses.float()\n",
        "          if hasattr(prompts, 'dtype') and prompts.dtype == torch.float16:\n",
        "              prompts = prompts.float()\n",
        "\n",
        "          # Turn off autocast during loss computation\n",
        "          with torch.amp.autocast(\"cuda\", enabled=False):\n",
        "              loss = self.compute_loss(prompts, responses, rewards)\n",
        "\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "\n",
        "          # Convert gradients to correct dtype and step the optimizer\n",
        "          for group in self.optimizer.param_groups:\n",
        "              for p in group['params']:\n",
        "                  if p.grad is not None:\n",
        "                      p.grad = p.grad.to(p.dtype)\n",
        "\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "          return {\"loss\": loss.item()}\n",
        "\n",
        "    def compute_loss(self, prompt_ids, response_ids, rewards):\n",
        "        pad_id = self.processing_class.pad_token_id\n",
        "        attention_mask = (response_ids != pad_id).to(response_ids.device, dtype=torch.long)\n",
        "\n",
        "        outputs = self.policy_model(\n",
        "            input_ids=response_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "        if isinstance(outputs, tuple):\n",
        "            logits = outputs[0]\n",
        "            value_predictions = outputs[1] if len(outputs) > 1 else None\n",
        "        else:\n",
        "            logits = outputs.logits\n",
        "            value_predictions = outputs.value if hasattr(outputs, \"value\") else None\n",
        "\n",
        "        new_policy_log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "        new_log_probs = new_policy_log_probs.gather(\n",
        "            dim=-1, index=response_ids.unsqueeze(-1)\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            cpu_response_ids = response_ids.to(\"cpu\")\n",
        "            cpu_attention_mask = attention_mask.to(\"cpu\")\n",
        "            ref_outputs = self.ref_model(\n",
        "                input_ids=cpu_response_ids,\n",
        "                attention_mask=cpu_attention_mask,\n",
        "                use_cache=False\n",
        "            )\n",
        "\n",
        "        if isinstance(ref_outputs, tuple):\n",
        "            ref_logits = ref_outputs[0]\n",
        "        else:\n",
        "            ref_logits = ref_outputs.logits\n",
        "\n",
        "        ref_logits = ref_logits.to(response_ids.device)\n",
        "        old_policy_log_probs = torch.nn.functional.log_softmax(ref_logits, dim=-1)\n",
        "        old_log_probs = old_policy_log_probs.gather(\n",
        "            dim=-1, index=response_ids.unsqueeze(-1)\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        ratios = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=ratios.device)\n",
        "        if rewards_tensor.dim() == 1:\n",
        "            advantage = rewards_tensor.unsqueeze(1).expand_as(ratios)\n",
        "        else:\n",
        "            advantage = rewards_tensor.expand_as(ratios)\n",
        "\n",
        "        clip_epsilon = 0.2\n",
        "        clipped_ratios = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon)\n",
        "        surrogate_loss = torch.min(ratios * advantage, clipped_ratios * advantage)\n",
        "        loss_policy = -torch.mean(surrogate_loss)\n",
        "\n",
        "        if value_predictions is not None:\n",
        "            loss_value = torch.nn.functional.mse_loss(value_predictions.squeeze(-1), rewards_tensor)\n",
        "        else:\n",
        "            loss_value = 0.0\n",
        "\n",
        "        kl_div = torch.nn.functional.kl_div(\n",
        "            new_policy_log_probs, old_policy_log_probs.exp(), reduction='batchmean'\n",
        "        )\n",
        "        kl_coef = 0.1\n",
        "        loss = loss_policy + loss_value + kl_coef * kl_div\n",
        "\n",
        "        # Debug logging to show intermediate loss values\n",
        "        log.info(f\"compute_loss: loss_policy={loss_policy.item():.4f}, loss_value={loss_value:.4f}, kl_div={kl_div.item():.4f}, total_loss={loss.item():.4f}\")\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "# Use the custom trainer if needed\n",
        "ppo_trainer = CustomPPOTrainer(\n",
        "    args=ppo_config,\n",
        "    model=model,\n",
        "    ref_model=reference_model,\n",
        "    reward_model=evaluate_contract,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IkT0BNW4t62"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_new_tokens=2000, temperature=0.7):\n",
        "    \"\"\"Generate text using the official Qwen pattern\"\"\"\n",
        "    # Format messages properly\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Solidity developer specialized in writing secure, efficient smart contracts. \"+\n",
        "            \"These contracts are intended to test your capabilities so ensure that you always provide complete, working code that compiles with no placeholders. \"\n",
        "            \"If there are values that you don't know, for example what to set an integer value to, just set it to any number. If you need a wallet id to accompilsh a task use GBH4TZYZ4IRCPO44CBOLFUHULU2WGALXTAVESQA6432MBJMABBB4GIYI\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    # Apply chat template as shown in quickstart\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Create inputs\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    # Generate with proper settings\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    # Extract only the new tokens - this is key!\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode only the response portion\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV_wSPxSO9zU"
      },
      "outputs": [],
      "source": [
        "def extract_contract_code(response_text):\n",
        "    \"\"\"Extract the contract code from model response with improved pattern matching.\n",
        "    Priority order:\n",
        "    1. Find the last code block (```solidity ... ```)\n",
        "    2. Look for Begin/End Smart Contract markers\n",
        "    \"\"\"\n",
        "    # Case 1: Look for the LAST code block with optional 'solidity' language marker\n",
        "    code_blocks = re.findall(r\"```(?:solidity)?\\s*(.*?)```\", response_text, re.DOTALL)\n",
        "\n",
        "    if code_blocks:\n",
        "        # Get the last code block (highest priority)\n",
        "        last_block = code_blocks[-1]\n",
        "        if \"contract\" in last_block:\n",
        "            start_marker = \"// Begin Smart Contract\"\n",
        "            end_marker = \"// End Smart Contract\"\n",
        "            return f\"{start_marker}\\n{last_block}\\n{end_marker}\"\n",
        "\n",
        "    # Case 2: Look for markers outside or inside code blocks - use LAST occurrence\n",
        "    start_marker = \"// Begin Smart Contract\"\n",
        "    end_marker = \"// End Smart Contract\"\n",
        "\n",
        "    # Find the LAST occurrence of the start marker\n",
        "    start_idx = response_text.rfind(start_marker)\n",
        "\n",
        "    # Find the LAST occurrence of the end marker\n",
        "    end_idx = response_text.rfind(end_marker)\n",
        "\n",
        "    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:\n",
        "        # Check if there's a closing bracket on the next line after the end marker\n",
        "        end_marker_pos = end_idx + len(end_marker)\n",
        "        next_closing_bracket = response_text[end_marker_pos:].find(\"}\")\n",
        "\n",
        "        if next_closing_bracket != -1 and response_text[end_marker_pos:end_marker_pos+next_closing_bracket].strip() == \"\":\n",
        "            # Include the closing bracket if it's on the next line\n",
        "            return response_text[start_idx:end_marker_pos + next_closing_bracket + 1]\n",
        "        else:\n",
        "            return response_text[start_idx:end_marker_pos]\n",
        "\n",
        "    # Case 3: No markers and no code blocks\n",
        "    log.info(\"No valid Solidity contract found in response\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-OihAqGnPMC"
      },
      "outputs": [],
      "source": [
        "# Function to clean up old checkpoints\n",
        "def cleanup_old_checkpoints(checkpoint_dir=\"/content/drive/MyDrive/checkpoints\",keep_last_n=1):\n",
        "    # Get all checkpoint files sorted by creation time (newest first)\n",
        "    all_checkpoints = sorted(\n",
        "        glob.glob(f\"{checkpoint_dir}/checkpoint_*.pt\"),\n",
        "        key=os.path.getctime,\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Get all model directories sorted by creation time (newest first)\n",
        "    all_model_dirs = sorted(\n",
        "        [d for d in glob.glob(f\"{checkpoint_dir}/model_epoch_*_prompt_*\") if os.path.isdir(d)],\n",
        "        key=os.path.getctime,\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Keep the N newest checkpoints, delete the rest\n",
        "    if len(all_checkpoints) > keep_last_n:\n",
        "        for old_checkpoint in all_checkpoints[keep_last_n:]:\n",
        "            try:\n",
        "                os.remove(old_checkpoint)\n",
        "                log.info(f\"Deleted old checkpoint: {old_checkpoint}\")\n",
        "            except Exception as e:\n",
        "                log.warning(f\"Failed to delete {old_checkpoint}: {e}\")\n",
        "\n",
        "    # Keep the N newest model directories, delete the rest\n",
        "    if len(all_model_dirs) > keep_last_n:\n",
        "        for old_model_dir in all_model_dirs[keep_last_n:]:\n",
        "            try:\n",
        "                shutil.rmtree(old_model_dir)\n",
        "                log.info(f\"Deleted old model directory: {old_model_dir}\")\n",
        "            except Exception as e:\n",
        "                log.warning(f\"Failed to delete {old_model_dir}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z79GS3jkmqUX"
      },
      "outputs": [],
      "source": [
        "def apply_token_level_rewards(contract_code, error_lines, overall_reward, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenize the extracted Solidity contract (which exactly matches the compiled file)\n",
        "    and assign a reward to each token based on whether its line number is in error_lines.\n",
        "    \"\"\"\n",
        "    # Tokenize the contract code (only the contract, not any extra prompt text)\n",
        "    contract_tokens = tokenizer(contract_code, return_tensors=\"pt\").input_ids[0]\n",
        "    token_rewards = []\n",
        "    current_line_num = 1\n",
        "\n",
        "    # Process each token, updating the current line count based on newline characters\n",
        "    for token_id in contract_tokens:\n",
        "        token_text = tokenizer.decode([token_id])\n",
        "        # If this token belongs to an error line, assign a negative reward\n",
        "        if current_line_num in error_lines:\n",
        "            token_rewards.append(-10)\n",
        "        else:\n",
        "            # Use a small positive reward if overall reward is positive; otherwise neutral (0)\n",
        "            token_rewards.append(0 if overall_reward < 0 else 1)\n",
        "        # Increase the current line count by the number of newline characters in the token\n",
        "        current_line_num += token_text.count('\\n')\n",
        "\n",
        "    # Sanity check: ensure token_rewards length matches the number of tokens\n",
        "    assert len(token_rewards) == len(contract_tokens), \"Mismatch in tokens and reward mapping!\"\n",
        "    return contract_tokens.unsqueeze(0), token_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgz0Gy8KYwLk"
      },
      "outputs": [],
      "source": [
        "def validate_prompt(prompt, model, tokenizer):\n",
        "    \"\"\"Run the model on a validation prompt and return the reward and error flag (1 if error, 0 if not).\"\"\"\n",
        "    full_response = generate_text(prompt, max_new_tokens=2000, temperature=0.7)\n",
        "    log.info(\"Validation full response:\\n\" + full_response)\n",
        "    contract_code = extract_contract_code(full_response)\n",
        "    if not contract_code:\n",
        "        log.info(\"No contract code extracted for validation prompt\")\n",
        "        return -10, 1\n",
        "    eval_result = evaluate_contract_hard(contract_code, full_response)\n",
        "    overall_reward = eval_result[\"reward\"]\n",
        "    error_flag = 1 if overall_reward < 0 else 0\n",
        "    log.info(f\"Validation evaluation result: reward = {overall_reward}\")\n",
        "    return overall_reward, error_flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg71vahoM1uh"
      },
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    \"\"\"Find and load the latest checkpoint if available.\"\"\"\n",
        "    checkpoint_files = glob.glob(f\"{checkpoint_dir}/checkpoint_*.pt\")\n",
        "    if not checkpoint_files:\n",
        "        return None, 0, 0, [], 0\n",
        "\n",
        "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
        "    match = re.search(r'checkpoint_epoch_(\\d+)_prompt_(\\d+)', latest_checkpoint)\n",
        "\n",
        "    if not match:\n",
        "        return None, 0, 0, [], 0\n",
        "\n",
        "    starting_epoch = int(match.group(1))\n",
        "    starting_prompt_idx = int(match.group(2)) + 1  # Start from the next prompt\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(latest_checkpoint)\n",
        "    compilation_errors = checkpoint.get('compilation_errors', [])\n",
        "    global_step = checkpoint.get('global_step', 0)\n",
        "\n",
        "    print(f\"Resuming from checkpoint: {latest_checkpoint}\")\n",
        "    print(f\"Starting from epoch {starting_epoch+1}, prompt {starting_prompt_idx+1}\")\n",
        "\n",
        "    return latest_checkpoint, starting_epoch, starting_prompt_idx, compilation_errors, global_step\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, ppo_trainer):\n",
        "    \"\"\"Load model and optimizer state from checkpoint.\"\"\"\n",
        "    if not checkpoint_path:\n",
        "        return\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    ppo_trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "def process_prompt(prompt, model, tokenizer, ppo_trainer, epoch, prompt_idx, compilation_errors):\n",
        "    # Generate full response from the model.\n",
        "    full_response = generate_text(prompt, max_new_tokens=2000, temperature=0.7)\n",
        "    log.info(\"Full response:\\n\" + full_response)\n",
        "\n",
        "    # Extract the contract code from the full response.\n",
        "    contract_code = extract_contract_code(full_response)\n",
        "    if not contract_code:\n",
        "        log.info(\"No contract code extracted, assigning negative reward\")\n",
        "        if epoch < len(compilation_errors):\n",
        "            compilation_errors[epoch] += 1\n",
        "        return -10\n",
        "    log.info(\"Extracted contract code:\\n\" + contract_code)\n",
        "\n",
        "    # Evaluate the contract by compiling it, installing dependencies, etc.\n",
        "    # This function now returns a dictionary with keys: \"reward\" and \"error_token_indices\".\n",
        "    eval_result = evaluate_contract_hard(contract_code, full_response)\n",
        "    overall_reward = eval_result[\"reward\"]\n",
        "    error_token_indices = eval_result[\"error_token_indices\"]\n",
        "    log.info(f\"Evaluation result: reward = {overall_reward}, error token indices = {error_token_indices}\")\n",
        "\n",
        "    # Prepare the prompt inputs.\n",
        "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    # Tokenize the extracted contract code exactly as compiled.\n",
        "    contract_tokens = tokenizer(contract_code, return_tensors=\"pt\").input_ids[0]\n",
        "    # Build token-level rewards based on error token indices.\n",
        "    token_rewards = []\n",
        "    for i, token_id in enumerate(contract_tokens):\n",
        "        if i in error_token_indices:\n",
        "            token_rewards.append(-1)\n",
        "        else:\n",
        "            # If overall_reward is negative, assign 0; if positive, assign a small positive reward.\n",
        "            token_rewards.append(0 if overall_reward < 0 else 0.01)\n",
        "    assert len(token_rewards) == contract_tokens.shape[0], \"Mismatch between token count and reward vector length\"\n",
        "\n",
        "    # Update the model using a sliding-window approach over the tokenized contract.\n",
        "    update_model_with_sliding_window(prompt_inputs, contract_tokens.unsqueeze(0).to(\"cuda:0\"), token_rewards, ppo_trainer)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return overall_reward\n",
        "\n",
        "\n",
        "\n",
        "def update_model_with_sliding_window(inputs, contract_tokens, token_rewards, ppo_trainer):\n",
        "    \"\"\"\n",
        "    Update the model using a sliding window approach, computing the average token-level reward\n",
        "    for each window and then updating the model via PPO. Extra logging provides visibility into\n",
        "    how the error rewards affect the loss.\n",
        "    \"\"\"\n",
        "    window_size = 256\n",
        "    step_size = 128  # 50% overlap\n",
        "\n",
        "    for start_idx in range(0, contract_tokens.size(1), step_size):\n",
        "        end_idx = min(start_idx + window_size, contract_tokens.size(1))\n",
        "        if end_idx - start_idx < 32:\n",
        "            continue  # Skip very short windows\n",
        "\n",
        "        window_ids = contract_tokens[:, start_idx:end_idx].to(\"cuda:0\")\n",
        "        window_token_rewards = token_rewards[start_idx:end_idx]\n",
        "        avg_window_reward = sum(window_token_rewards) / len(window_token_rewards)\n",
        "\n",
        "        stats = ppo_trainer.step(\n",
        "            inputs.input_ids,\n",
        "            window_ids,\n",
        "            [avg_window_reward]\n",
        "        )\n",
        "        log.info(f\"Window {start_idx}-{end_idx}: avg_token_reward = {avg_window_reward:.4f}, loss = {stats.get('loss', 0):.4f}\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def update_model_with_sliding_window(inputs, contract_tokens, token_rewards, ppo_trainer):\n",
        "    \"\"\"\n",
        "    Update the model using a sliding window approach where the reward for each window\n",
        "    is computed as the average of the token-level rewards. Extra logging is added\n",
        "    so you can see the average reward and the resulting loss for each window.\n",
        "    \"\"\"\n",
        "    window_size = 256\n",
        "    step_size = 128  # 50% overlap between windows\n",
        "\n",
        "    for start_idx in range(0, contract_tokens.size(1), step_size):\n",
        "        end_idx = min(start_idx + window_size, contract_tokens.size(1))\n",
        "        if end_idx - start_idx < 32:  # Skip very small chunks\n",
        "            continue\n",
        "\n",
        "        window_ids = contract_tokens[:, start_idx:end_idx].to(\"cuda:0\")\n",
        "        # Compute the average reward over tokens in this window\n",
        "        window_token_rewards = token_rewards[start_idx:end_idx]\n",
        "        avg_window_reward = sum(window_token_rewards) / len(window_token_rewards)\n",
        "\n",
        "        stats = ppo_trainer.step(\n",
        "            inputs.input_ids,\n",
        "            window_ids,\n",
        "            [avg_window_reward]\n",
        "        )\n",
        "\n",
        "        # Log additional details for visibility\n",
        "        log.info(f\"Window {start_idx}-{end_idx}: avg_token_reward: {avg_window_reward:.4f}, loss: {stats.get('loss', 0):.4f}\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, epoch, prompt_idx, global_step, model, ppo_trainer, compilation_errors):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    checkpoint_path = f\"{checkpoint_dir}/checkpoint_epoch_{epoch}_prompt_{prompt_idx}.pt\"\n",
        "    torch.save({\n",
        "        'global_step': global_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': ppo_trainer.optimizer.state_dict(),\n",
        "        'compilation_errors': compilation_errors,\n",
        "    }, checkpoint_path)\n",
        "    log.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    # Optionally save in HF format as well (includes both base model and value head)\n",
        "    model_save_path = f\"{checkpoint_dir}/model_epoch_{epoch}_prompt_{prompt_idx}\"\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    log.info(f\"Saved model to {model_save_path}\")\n",
        "\n",
        "    # Clean up old checkpoints after successful save\n",
        "    cleanup_old_checkpoints(checkpoint_dir=checkpoint_dir)\n",
        "\n",
        "\n",
        "def save_final_model(checkpoint_dir, model, tokenizer, ppo_trainer, compilation_errors, global_step):\n",
        "    \"\"\"Save the final model and generate performance plot.\"\"\"\n",
        "    # Save final checkpoint\n",
        "    final_checkpoint_path = f\"{checkpoint_dir}/checkpoint_final.pt\"\n",
        "    torch.save({\n",
        "        'global_step': global_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': ppo_trainer.optimizer.state_dict(),\n",
        "        'compilation_errors': compilation_errors,\n",
        "    }, final_checkpoint_path)\n",
        "    log.info(f\"Saved final checkpoint to {final_checkpoint_path}\")\n",
        "\n",
        "    # Save final model in HF format\n",
        "    model.save_pretrained(f\"{checkpoint_dir}/model_final\")\n",
        "    tokenizer.save_pretrained(f\"{checkpoint_dir}/model_final\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_performance(num_epochs, training_errors, validation_errors):\n",
        "    \"\"\"Plot both training and validation compilation errors per epoch with data point labels.\"\"\"\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, training_errors, marker='o', linestyle='-', label='Training Compilation Errors')\n",
        "    plt.plot(epochs, validation_errors, marker='o', linestyle='-', label='Validation Compilation Errors')\n",
        "    # Annotate each data point with its value.\n",
        "    for i, (train_err, val_err) in enumerate(zip(training_errors, validation_errors)):\n",
        "        plt.text(i+1, train_err, str(train_err), ha='center', va='bottom')\n",
        "        plt.text(i+1, val_err, str(val_err), ha='center', va='bottom')\n",
        "    plt.title('Compilation Errors per Epoch')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Number of Compilation Errors')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(epochs)\n",
        "    plt.savefig('/content/drive/MyDrive/compilation_errors.png')\n",
        "    plt.show()\n",
        "\n",
        "def train(prompts, val_prompts, num_epochs=5, save_every=20, checkpoint_dir=\"/content/drive/MyDrive/checkpoints\"):\n",
        "    \"\"\"Main training function updated to use a validation set and log when a prompt that previously failed now compiles.\n",
        "    Also tracks and returns training and validation compilation errors.\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Tracking variables for each epoch.\n",
        "    compilation_errors = [0 for _ in range(num_epochs)]\n",
        "    val_errors = []  # To store the number of validation compilation errors per epoch\n",
        "    global_step = 0\n",
        "\n",
        "    # Track compile status for each prompt over epochs (None means not set).\n",
        "    prev_compile_status = [None] * len(prompts)\n",
        "\n",
        "    latest_checkpoint, starting_epoch, starting_prompt_idx, loaded_errors, loaded_step = find_latest_checkpoint(checkpoint_dir)\n",
        "    if latest_checkpoint:\n",
        "        load_checkpoint(latest_checkpoint, model, ppo_trainer)\n",
        "        compilation_errors = loaded_errors if loaded_errors else compilation_errors\n",
        "        global_step = loaded_step\n",
        "\n",
        "    try:\n",
        "        for epoch in range(starting_epoch, num_epochs):\n",
        "            log.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            current_compile_status = [None] * len(prompts)\n",
        "\n",
        "            # Process training prompts.\n",
        "            start_idx = starting_prompt_idx if epoch == starting_epoch else 0\n",
        "            for i, prompt in enumerate(prompts[start_idx:], start=start_idx):\n",
        "                global_step += 1\n",
        "                log.info(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
        "                overall_reward = process_prompt(prompt, model, tokenizer, ppo_trainer, epoch, i, compilation_errors)\n",
        "                # Mark prompt as 'compiled' (True) if reward is positive.\n",
        "                current_compile_status[i] = (overall_reward > 0)\n",
        "                if overall_reward < 0:\n",
        "                    compilation_errors[epoch] += 1\n",
        "                if global_step % save_every == 0:\n",
        "                    save_checkpoint(checkpoint_dir, epoch, i, global_step, model, ppo_trainer, compilation_errors)\n",
        "\n",
        "            print(f\"Number of training compilation errors for this epoch: {compilation_errors[epoch]}\")\n",
        "\n",
        "            # Log if any prompt that failed in the previous epoch now compiles.\n",
        "            if prev_compile_status[0] is not None:\n",
        "                for i, (prev_status, curr_status) in enumerate(zip(prev_compile_status, current_compile_status)):\n",
        "                    if prev_status is False and curr_status is True:\n",
        "                        log.info(f\"Prompt {i+1} now compiles but previously did not.\")\n",
        "            prev_compile_status = current_compile_status\n",
        "\n",
        "            # Evaluate on the validation set.\n",
        "            epoch_val_errors = 0\n",
        "            if val_prompts:\n",
        "                for val_prompt in val_prompts:\n",
        "                    _, error_flag = validate_prompt(val_prompt, model, tokenizer)\n",
        "                    epoch_val_errors += error_flag\n",
        "                log.info(f\"Epoch {epoch+1} validation compilation errors: {epoch_val_errors}\")\n",
        "            else:\n",
        "                epoch_val_errors = 0\n",
        "            val_errors.append(epoch_val_errors)\n",
        "    finally:\n",
        "        save_final_model(checkpoint_dir, model, tokenizer, ppo_trainer, compilation_errors, global_step)\n",
        "        plot_performance(num_epochs, compilation_errors, val_errors)\n",
        "    return compilation_errors, val_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfexy_AfkoTM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  train(prompts,validation_prompts)\n",
        "finally:\n",
        "  import shutil\n",
        "  shutil.copy(\"/content/logger.log\",\"/content/drive/MyDrive/log.log\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "1Vc7Kabl4Eg1NFcg71otOQnQx-UeIGfBI",
      "authorship_tag": "ABX9TyMBhOkNrBFg0+01nE6xzaz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}